# DB-Transform Implementation Guide

**Date**: 2025-12-01
**Status**: ‚úÖ Implementation Complete - Ready for Training

---

## üéØ Quick Start

### 1. Train DB-Transform (Default)

```bash
cd /ghome/fewahab/Sun-Models/Ab-5/CMGAN/Modified
python train.py
```

The configuration is already set to use DB-Transform by default.

### 2. Switch to BSRNN Baseline

To run baseline BSRNN for comparison:

```python
# In Modified/train.py, line 23:
model_type = 'BSRNN'  # Change from 'DB_Transform' to 'BSRNN'
```

---

## üìÅ File Structure

### New Files Created:
```
Modified/
‚îú‚îÄ‚îÄ db_transform.py          # ‚úÖ Complete DB-Transform implementation
‚îî‚îÄ‚îÄ train.py                 # ‚úÖ Updated with model selection

Root/
‚îú‚îÄ‚îÄ LITERATURE_GROUNDED_PROPOSAL.md    # Full theoretical background
‚îú‚îÄ‚îÄ CRITICAL_ANALYSIS_Net1_vs_Net2.md  # Initial analysis
‚îî‚îÄ‚îÄ DB_TRANSFORM_IMPLEMENTATION.md     # This file
```

---

## üèóÔ∏è Architecture Overview

### DB-Transform Components:

| Component | Purpose | Literature Source | Parameters |
|-----------|---------|-------------------|------------|
| **BandSplit** | 30 frequency bands | BSRNN (Yu et al. 2023) | 200K |
| **DifferentialBandAttention** | Band-wise noise canceling attention | Microsoft (Oct 2024) | 2.1M |
| **FrequencyAdaptiveMultiScale** | Psychoacoustic multi-scale fusion | Li et al. (2023) | 0.6M |
| **LearnableHarmonicGraph** | Inter-band harmonic modeling | GNN Audio (2023-24) | 0.9M |
| **MaskDecoder** | 3-tap FIR filter prediction | BSRNN (Yu et al. 2023) | 0.3M |
| **Total** | | | **4.1M** |

### Data Flow:
```
Complex Spectrogram [B, 257, T]
    ‚Üì
BandSplit ‚Üí [B, N, T, 30]
    ‚Üì
Differential Attention ‚Üí [B, N, T, 30] (+ Residual)
    ‚Üì
Multi-Scale Fusion ‚Üí [B, N, T, 30] (+ Residual)
    ‚Üì
Harmonic Graph ‚Üí [B, N, T, 30] (+ Residual)
    ‚Üì
MaskDecoder ‚Üí Complex Spectrogram [B, 257, T]
```

---

## ‚öôÔ∏è Configuration

### Current Settings (Modified/train.py):

```python
class Config:
    # Model Selection
    model_type = 'DB_Transform'  # or 'BSRNN'

    # Training Hyperparameters
    epochs = 120
    batch_size = 6
    log_interval = 500
    decay_epoch = 10
    init_lr = 1e-3

    # Paths
    data_dir = '/gdata/fewahab/data/VoicebanK-demand-16K'
    save_model_dir = '/ghome/fewahab/Sun-Models/Ab-5/CMGAN/saved_model_dbtransform'
```

### Model-Specific Parameters:

**DB-Transform**:
- `num_channel=128` - Feature dimension per band
- `num_heads=4` - Number of attention heads
- Expected params: ~4.1M

**BSRNN (Baseline)**:
- `num_channel=64` - Feature dimension per band
- `num_layer=5` - Number of LSTM layers
- Expected params: ~2.8M

---

## üöÄ Training Instructions

### Step 1: Verify Environment

```bash
# Check GPU availability
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
python -c "import torch; print(f'GPU: {torch.cuda.get_device_name(0)}')"

# Check dataset
ls -lh /gdata/fewahab/data/VoicebanK-demand-16K/
```

### Step 2: Start Training

```bash
cd /ghome/fewahab/Sun-Models/Ab-5/CMGAN/Modified

# Full training (120 epochs)
python train.py

# Or with nohup for long runs
nohup python train.py > train_dbtransform.log 2>&1 &
tail -f train_dbtransform.log
```

### Step 3: Monitor Training

The training will log:
- Generator loss (L1 magnitude + L1 RI + GAN loss)
- Discriminator loss (Metric discriminator)
- **PESQ score** (denormalized, -0.5 to 4.5 scale)

Example output:
```
INFO:root:Using DB-Transform architecture (4.1M parameters)
INFO:root:Model parameters: Total=4.12M, Trainable=4.12M
INFO:root:Epoch 0, Step 500, loss: 0.0234, disc_loss: 0.0012, PESQ: 2.45
INFO:root:TEST - Generator loss: 0.0198, Discriminator loss: 0.0009, PESQ: 2.67
```

### Step 4: Training Phases

The training has two phases:

**Phase 1 (Epochs 0-59)**: Generator only
- No discriminator updates
- Focus on magnitude and RI reconstruction

**Phase 2 (Epochs 60-119)**: Generator + Discriminator
- Metric discriminator activated
- GAN training for perceptual quality

---

## üìä Expected Results

### Performance Predictions (from Literature Analysis):

| Model | PESQ | Parameters | FLOPs/frame | RTF |
|-------|------|------------|-------------|-----|
| **BSRNN (Baseline)** | 3.10 | 2.8M | 3.2 GFLOPs | 0.42 |
| **DB-Transform (Conservative)** | 3.25-3.30 | 4.1M | 4.8 GFLOPs | ~0.55 |
| **DB-Transform (Optimistic)** | 3.30-3.35 | 4.1M | 4.8 GFLOPs | ~0.55 |
| CMGAN (SOTA) | 3.41 | 4.2M | 8.5 GFLOPs | 0.65 |

**Expected improvement over BSRNN**: +0.15 to +0.25 PESQ

### Decision Points:

**After Epoch 30** (Mid Phase 1):
- ‚úÖ PESQ > 2.50: Continue normally
- ‚ö†Ô∏è PESQ < 2.30: Check for issues (learning rate, gradients)

**After Epoch 60** (End Phase 1):
- ‚úÖ PESQ > 2.90: Continue to Phase 2
- ‚ö†Ô∏è PESQ < 2.70: Consider debugging

**After Epoch 120** (Final):
- ‚úÖ PESQ > 3.20: Success! Ready for ablations
- ‚ö†Ô∏è PESQ 3.00-3.20: Acceptable, may need hyperparameter tuning
- ‚ùå PESQ < 3.00: Debug required

---

## üî¨ Ablation Studies (After Initial Training)

To isolate each component's contribution:

### Ablation 1: Differential Attention Only

```python
# In Modified/db_transform.py, DB_Transform.forward():
# Comment out multi-scale and harmonic graph

# Stage 3: Multi-scale
# z_ms = self.multi_scale(z)
# z = self.norm2(z + z_ms)
z = self.norm2(z)  # No multi-scale

# Stage 4: Harmonic graph
# z_graph = self.harmonic_graph(z)
# z = self.norm3(z + z_graph)
z = self.norm3(z)  # No harmonic graph
```

**Expected**: PESQ ~3.18-3.22 (+0.08-0.12 over BSRNN)

### Ablation 2: + Multi-Scale (No Harmonic Graph)

```python
# Enable multi-scale, disable harmonic graph

# Stage 3: Multi-scale
z_ms = self.multi_scale(z)
z = self.norm2(z + z_ms)  # ‚úÖ Enabled

# Stage 4: Harmonic graph
# z_graph = self.harmonic_graph(z)
# z = self.norm3(z + z_graph)
z = self.norm3(z)  # ‚ùå Disabled
```

**Expected**: PESQ ~3.25-3.30 (+0.15-0.20 over BSRNN)

### Ablation 3: Full Model

All components enabled (default configuration).

**Expected**: PESQ ~3.30-3.35 (+0.20-0.25 over BSRNN)

---

## üîç Analysis & Visualization

### Extract Learned Harmonic Graph:

```python
import torch
from db_transform import DB_Transform

# Load trained model
model = DB_Transform(num_channel=128, num_heads=4)
model.load_state_dict(torch.load('saved_model_dbtransform/gene_epoch_119_...'))
model.eval()

# Visualize learned graph
adj_matrix, harmonic_pairs = model.harmonic_graph.visualize_learned_graph()

print(f"Discovered {len(harmonic_pairs)} harmonic relationships:")
for pair in harmonic_pairs:
    print(f"Band {pair['band_i']} ({pair['freq_i']:.1f} Hz) ‚Üî "
          f"Band {pair['band_j']} ({pair['freq_j']:.1f} Hz) "
          f"[~{pair['harmonic']}x harmonic, weight={pair['weight']:.3f}]")
```

### Extract Multi-Scale Fusion Weights:

```python
# Get learned fusion weights per band
for k in range(30):
    weights = model.multi_scale.fusion_weights[f'band_{k}']
    scales = model.multi_scale.scale_configs[k]['scales']

    print(f"Band {k}: {scales} ‚Üí weights {weights.detach().cpu().numpy()}")
```

This will show which temporal scales each frequency band prefers (should correlate with psychoacoustic priors).

---

## üêõ Troubleshooting

### Issue 1: CUDA Out of Memory

**Symptom**: `RuntimeError: CUDA out of memory`

**Solutions**:
```python
# Reduce batch size
batch_size = 4  # or 3

# Or reduce model size
DB_Transform(num_channel=96, num_heads=4)  # Instead of 128
```

### Issue 2: NaN Loss

**Symptom**: Loss becomes NaN after few iterations

**Solutions**:
```python
# Reduce learning rate
init_lr = 5e-4  # Instead of 1e-3

# Check gradient clipping (already enabled)
# In train_step: torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=5)
```

### Issue 3: Slow Training

**Symptom**: Training much slower than BSRNN

**Expected**: DB-Transform is ~30% slower (more parameters, more computation)

**Solutions**:
- Reduce `num_heads` from 4 to 2
- Check GPU utilization: `nvidia-smi -l 1`
- Ensure using CUDA: Model shows "cuda:0" device

### Issue 4: Import Error for db_transform

**Symptom**: `ModuleNotFoundError: No module named 'db_transform'`

**Solution**:
```bash
# Ensure you're in the Modified directory
cd /ghome/fewahab/Sun-Models/Ab-5/CMGAN/Modified
python train.py
```

Or update Python path:
```python
import sys
sys.path.insert(0, '/ghome/fewahab/Sun-Models/Ab-5/CMGAN/Modified')
```

---

## üìà Next Steps After Training

### 1. Compare with BSRNN Baseline

Train BSRNN baseline on same data:
```python
# Modified/train.py
model_type = 'BSRNN'
save_model_dir = '/ghome/fewahab/Sun-Models/Ab-5/CMGAN/saved_model_bsrnn'
```

Compare final PESQ scores.

### 2. Run Full Ablation Studies

Train 3 ablation configurations (see Ablation Studies section above).

### 3. Extended Evaluation

Test on additional datasets:
- DNS Challenge dataset
- TIMIT + noise
- Real-world recordings

### 4. Analysis for Paper

Generate figures:
- Learned harmonic graph visualization
- Fusion weight analysis (compare with TMTF theory)
- Spectrogram comparisons (Noisy / BSRNN / DB-Transform)
- Ablation study bar chart

### 5. Paper Writing

Follow publication strategy in `LITERATURE_GROUNDED_PROPOSAL.md`:
- Target: IEEE TASLP / ICASSP 2026 / Interspeech 2025
- Title: "DB-Transform: Differential Band-Split Transformer with Learnable Harmonic Graph for Speech Enhancement"
- Key contributions:
  1. Band-wise differential attention
  2. Frequency-adaptive multi-scale fusion
  3. Learnable harmonic graph

---

## üìö References

See `LITERATURE_GROUNDED_PROPOSAL.md` for full literature review and citations.

**Key Papers**:
1. Yu et al. (2023) - BSRNN (Interspeech 2023)
2. Ye et al. (2024) - Differential Transformer (Microsoft, Oct 2024)
3. Li et al. (2023) - Multi-Scale Temporal Transformer (Interspeech 2023)
4. Cao et al. (2024) - CMGAN (IEEE TASLP 2024)

---

## ‚úÖ Implementation Checklist

- [x] DifferentialBandAttention module implemented
- [x] FrequencyAdaptiveMultiScale module implemented
- [x] LearnableHarmonicGraph module implemented
- [x] DB_Transform class integrated
- [x] Training pipeline updated
- [x] Configuration system added
- [x] Documentation created
- [ ] Initial training run (120 epochs)
- [ ] BSRNN baseline comparison
- [ ] Ablation studies
- [ ] Extended evaluation
- [ ] Paper writing

---

## üí° Key Implementation Details

### 1. Differential Attention per Band

Each of 30 frequency bands gets its own differential attention module with learned Œª (noise cancellation weight). This allows frequency-dependent noise modeling.

### 2. Psychoacoustic Scale Assignment

Multi-scale fusion uses different temporal scales per frequency band:
- Low freq (0-500 Hz): [8, 16, 32] frames (long windows)
- Mid freq (500-2000 Hz): [4, 8, 16] frames (medium windows)
- High freq (2000+ Hz): [2, 4, 8] frames (short windows)

Based on Temporal Modulation Transfer Function (TMTF) from psychoacoustics.

### 3. Sparse Harmonic Graph

Graph connectivity is learned but kept sparse (top-5 connections per band). This:
- Reduces computation (150 edges vs 900 in full graph)
- Improves interpretability (can identify harmonic relationships)
- Prevents overfitting (structured sparsity)

### 4. I/O Compatibility

DB-Transform is a **drop-in replacement** for BSRNN:
- Same input: Complex spectrogram [B, 257, T]
- Same output: Enhanced complex spectrogram [B, 257, T]
- Same STFT parameters: n_fft=512, hop=128

No changes needed to training loop, loss functions, or data loading.

---

**Ready to train!** üöÄ

Questions or issues? Check the troubleshooting section or refer to the literature proposal for theoretical details.
